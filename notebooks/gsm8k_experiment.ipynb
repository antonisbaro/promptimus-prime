{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76405687",
   "metadata": {},
   "source": [
    "# üìò Promptimus Prime: LLM-AutoDiff Reproduction\n",
    "\n",
    "## ü§ñ **LLM-AutoDiff: Auto-Differentiate Any LLM Workflow**\n",
    "\n",
    "Welcome to **Promptimus Prime**! This notebook reproduces the experiments from the paper *\"LLM-AutoDiff: Auto-Differentiate Any LLM Workflow\"*.\n",
    "\n",
    "We utilize **Textual Gradient Descent (TGD)** to automatically optimize system prompts for Large Language Models. Instead of manual prompt engineering, we treat the prompt as a trainable parameter and use a \"Teacher\" LLM to provide gradients (textual feedback) based on the \"Student\" LLM's errors.\n",
    "\n",
    "### üßÆ **The Task: GSM8K (Grade School Math)**\n",
    "*   **Goal:** Solve multi-step mathematical reasoning problems.\n",
    "*   **Student Model:** `Qwen2.5-1.5B-Instruct` (Lightweight, efficient).\n",
    "*   **Teacher Model:** `Qwen2.5-7B-Instruct` (Stronger reasoning capabilities).\n",
    "*   **Optimization:** We optimize the system prompt to improve the Student's Chain-of-Thought reasoning.\n",
    "\n",
    "### üõ†Ô∏è **Architecture**\n",
    "1.  **Forward Pass:** Student attempts to solve a math problem.\n",
    "2.  **Evaluation:** We check if the final answer matches the Ground Truth.\n",
    "3.  **Backward Pass:** If incorrect, the Teacher analyzes the error and generates a \"Textual Gradient\".\n",
    "4.  **Update:** The Optimizer refines the system prompt to fix the error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557d8d4",
   "metadata": {},
   "source": [
    "### üöÄ **Step 1: Setup & Installation**\n",
    "\n",
    "We start by cloning the **Promptimus Prime** repository. Then, we install all necessary dependencies defined in `requirements.txt` to ensure our environment matches the project specifications.\n",
    "\n",
    "**Note:** Ensure you are connected to a **GPU Runtime** (T4 is sufficient) before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ad893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Clone the repository\n",
    "!git clone https://github.com/antonisbaro/promptimus-prime.git\n",
    "\n",
    "# 2. Enter the project directory\n",
    "%cd promptimus-prime\n",
    "\n",
    "# 3. Install dependencies from requirements.txt\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e85649",
   "metadata": {},
   "source": [
    "We add the repository to the system path to allow direct imports. We also configure logging to suppress verbose output from libraries, ensuring that progress bars (tqdm) render correctly in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import transformers\n",
    "\n",
    "# Add the repository to Python path\n",
    "repo_path = \"/content/promptimus-prime\"\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "# Configure Global Logging (Silence the noise)\n",
    "# Force re-configuration to override Colab defaults\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "\n",
    "# Suppress specific library noise\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"adalflow\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "print(\"‚úÖ Environment configured for interactive execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14108fb5",
   "metadata": {},
   "source": [
    "### üîë **Step 2: Hugging Face Login (Optional)**\n",
    "\n",
    "If you plan to use gated models or want to avoid download limits, log in to Hugging Face. For `Qwen2.5`, this is usually not strictly required but recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata \n",
    "from huggingface_hub import login\n",
    "\n",
    "try:\n",
    "    # Ensure you have added 'HF_TOKEN' to your Colab Secrets\n",
    "    token = userdata.get('HF_TOKEN')\n",
    "    login(token)\n",
    "    print(\"‚úÖ Successfully logged in to Hugging Face!\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è HF_TOKEN not found in secrets. Continuing without authentication (some models may not work).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e18e08",
   "metadata": {},
   "source": [
    "### üß† **Step 3: Run Training (Optimization Loop)**\n",
    "\n",
    "We will now start the **Textual Gradient Descent** loop.\n",
    "*   **Train Split:** Used to generate gradients.\n",
    "*   **Validation Split:** Used to validate if the new prompt is actually better.\n",
    "\n",
    "The script `src.tasks.gsm8k.train` handles the entire pipeline: loading models, slicing data, and running the AdalFlow trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5562c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the main execution function and run it directly\n",
    "# This will load the models (4-bit), run the optimization steps, and save the result.\n",
    "from src.tasks.gsm8k.train import run_training # pyright: ignore[reportMissingImports]\n",
    "\n",
    "# Execute the training pipeline\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0bb68f",
   "metadata": {},
   "source": [
    "### üìä **Step 4: Final Evaluation**\n",
    "\n",
    "Now that we have an **Optimized Prompt**, let's compare it against the **Baseline (Zero-shot)** prompt on a held-out **Test Set**.\n",
    "\n",
    "This script will:\n",
    "1.  Run inference using the default prompt.\n",
    "2.  Run inference using the optimized prompt found in Step 3.\n",
    "3.  Report the accuracy improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f45f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the evaluation function and run it directly\n",
    "from src.tasks.gsm8k.evaluate import run_evaluation # pyright: ignore[reportMissingImports]\n",
    "\n",
    "# Execute the evaluation\n",
    "run_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac4736",
   "metadata": {},
   "source": [
    "### üìù **Step 5: Inspect the Optimized Prompt**\n",
    "\n",
    "Let's see what the \"Teacher\" taught the \"Student\". Here is the final system prompt that yielded the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b81a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "prompt_path = \"outputs/gsm8k/optimized_prompt.txt\"\n",
    "\n",
    "if os.path.exists(prompt_path):\n",
    "    print(\"\\n‚ú® \\033[1mFINAL OPTIMIZED PROMPT:\\033[0m\\n\" + \"=\"*40)\n",
    "    with open(prompt_path, \"r\") as f:\n",
    "        print(f.read())\n",
    "    print(\"=\"*40)\n",
    "else:\n",
    "    print(\"‚ùå Prompt file not found. Did the training finish successfully?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790b6df",
   "metadata": {},
   "source": [
    "### üìà **Step 6: Visualization & Analysis**\n",
    "\n",
    "We now visualize the improvements (success stories) and the evolution of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tasks.gsm8k.visualize import run_visualization\n",
    "\n",
    "run_visualization()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
